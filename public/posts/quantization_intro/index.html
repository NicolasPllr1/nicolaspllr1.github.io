<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>What is quantization in modern AI ? | Nicolas&#39; Blog</title>
<meta name="keywords" content="">
<meta name="description" content="What is quantization ? First definition Quantization refers to a broad class of algorithms. Their goal is to reduce the memory footprint of models while retaining as much performance as possible.
$$ \text{Quantization} \ \approx \ \searrow \ \text{memory footprint of models} $$
More precisely, this reduction in memory size will be achieved by compressing models weights - or parameters if you prefer.
Case study : 70B model Consider the new Llama 3 70B.">
<meta name="author" content="Nicolas Pellerin">
<link rel="canonical" href="https://nicolaspllr1.github.io/posts/quantization_intro/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://nicolaspllr1.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://nicolaspllr1.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://nicolaspllr1.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://nicolaspllr1.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://nicolaspllr1.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://nicolaspllr1.github.io/posts/quantization_intro/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

<script>
  console.log('Custom script loaded');
</script>


  <script type="text/javascript">
    MathJax = {
      tex: {
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        inlineMath: [['$', '$'], ['\\(', '\\)']],
      },
    };
  </script>
  <script
      async
      id="MathJax-script"
      src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"
      integrity="sha384-+BSz3oj3ILMYvOBr16U9i0H4RZRmGyQQ+1q9eqr8T3skmAFrJk8GmgwgqlCZdNSo"
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
      type="text/javascript"></script>

  

<meta property="og:title" content="What is quantization in modern AI ?" />
<meta property="og:description" content="What is quantization ? First definition Quantization refers to a broad class of algorithms. Their goal is to reduce the memory footprint of models while retaining as much performance as possible.
$$ \text{Quantization} \ \approx \ \searrow \ \text{memory footprint of models} $$
More precisely, this reduction in memory size will be achieved by compressing models weights - or parameters if you prefer.
Case study : 70B model Consider the new Llama 3 70B." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://nicolaspllr1.github.io/posts/quantization_intro/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-05-23T20:31:19+02:00" />
<meta property="article:modified_time" content="2024-05-23T20:31:19+02:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="What is quantization in modern AI ?"/>
<meta name="twitter:description" content="What is quantization ? First definition Quantization refers to a broad class of algorithms. Their goal is to reduce the memory footprint of models while retaining as much performance as possible.
$$ \text{Quantization} \ \approx \ \searrow \ \text{memory footprint of models} $$
More precisely, this reduction in memory size will be achieved by compressing models weights - or parameters if you prefer.
Case study : 70B model Consider the new Llama 3 70B."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://nicolaspllr1.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "What is quantization in modern AI ?",
      "item": "https://nicolaspllr1.github.io/posts/quantization_intro/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "What is quantization in modern AI ?",
  "name": "What is quantization in modern AI ?",
  "description": "What is quantization ? First definition Quantization refers to a broad class of algorithms. Their goal is to reduce the memory footprint of models while retaining as much performance as possible.\n$$ \\text{Quantization} \\ \\approx \\ \\searrow \\ \\text{memory footprint of models} $$\nMore precisely, this reduction in memory size will be achieved by compressing models weights - or parameters if you prefer.\nCase study : 70B model Consider the new Llama 3 70B.",
  "keywords": [
    
  ],
  "articleBody": " What is quantization ? First definition Quantization refers to a broad class of algorithms. Their goal is to reduce the memory footprint of models while retaining as much performance as possible.\n$$ \\text{Quantization} \\ \\approx \\ \\searrow \\ \\text{memory footprint of models} $$\nMore precisely, this reduction in memory size will be achieved by compressing models weights - or parameters if you prefer.\nCase study : 70B model Consider the new Llama 3 70B.\nIf you load it in 32-bit precision on your GPU 1, it will require around (32 / 8) * 70 = 280 GB of VRAM ü•∂. For regular folks with access to 1 or 2 consumer GPUs at best, this memory cost is prohibitive and the model cannot be run 2.\nThis is where quantization kicks in to lower the memory required to run models.\nBefore we dive into data types, memory footprint calculations and specific quantization algorithms such as GPTQ, AWQ or HQQ, let‚Äôs understand why quantization in particular and model compression in general have become critical in modern AI.\nModern context Plus-size AI AI models have been getting larger and larger.\nEpoch AI has a nice paper exploring this trend + their dataset is regularly updated. Look at the evolution of the parameter count from 1950 to 2024 :\nPay attention to the y-axis log scale. It indicates the parameter count order of magnitude - OOM. Notice how the slope is increasing. On this figure, every vertical increment represent a gain of a full order of magnitude !\nMeaning across the board, models have been gaining orders of magnitudes more parameters and this increase in size has been accelerating\nNatural language processing Consider the GPT series trained by OpenAI :\nQuick drawing I made - these GPTs sizes are well know as they were disclosed in OpenAI‚Äôs technical reports/papers/blog posts\nGaining 3 OOMs was achieved in ~ 2 years.\nQuick drawing I made\nMid 2018, OpenAI released GPT-1 with ~100M params 3. Next in 2019, they progressively released the GPT-2 models. The largest GPT-2 at 1.5B params was 10x larger than GPT-1 . Next mid 2020 they announced GPT-3 - but did not release the weights this time üò¢ - with an outstanding size of 175B parameters. Meaning ~100x GPT-2 size and 10x larger than any dense model at the time. Finally in March 2024 they announced GPT-4. Although this time they did not publicly disclose its architecture, it is rumored to be a 1.7T MoE so again a 10x increase in size compared to GPT-3 4.\nAnd it‚Äôs not just OpenAI‚Äôs GPTs. This trend can be observed across the board in the modern NLP history :\nYear Model Parameter Count 1997 Original LSTM ~10k 2014 Seq2Seq LSTM ~400M 2017 Original Transformer ~200M 2018 BERT Base - Large 110M - 340M Early 2023 Llama 1 series 7B, 13B, 33B, 65B Summer 2023 Llama 2 series 7B, 13B, 70B April 2024 Llama 3 series 8B, 70B, 400B (not released yet) In 2024, almost all models have a parameter count in the billions ! Models having less than 10 billions parameters are often considered ‚Äúsmall‚Äù ! For instance, the release of llama 3 8B has been much appreciated as its size makes it relatively small in the modern context of LLMs and it has been trained a lot. Making smaller models more capable is a recent trend that seems to be gaining traction. Andrej Karpathy for one has been pushing for smaller yet capable models and the recent Phi-3 model for example drives in this direction.\nComputer vision Same thing in computer vision although the latest models are not as large as the largest LLMs. Let‚Äôs compress aggressively modern computer vision history into a couple of milestone models : LeNet ‚Äî\u003eAlexNet ‚Äî\u003e ResNets, YOLOs ‚Äî\u003e ViTs\nTimeline I made. See the increasing number of parameters :)\nIn short, models sizes across the board in the AI landscape have been exploding. And why you may ask ?\nScaling laws One major force that is driving this explosion is scaling laws.\nI won‚Äôt deep dive into this fascinating topic here - maybe in a future post :) .\nIf you want to get started, EpochAI literature review looks great 5.\nOn a personal note, I would recommend paper-wise reading OpenAI‚Äôs 2020 publications where they study scaling laws for GPTs performing language modeling up to 1B models, and then explore how their findings generalize to other AI tasks.\nScaling laws beautifully visualized - in OpenAI‚Äôs first paper mentioned above\nFinally, the 2022 Chinchilla paper which provides more precise scaling laws than OpenAI‚Äôs and estimates ‚Äúcompute optimal‚Äù training recipes.\nAnd obviously you should read Dwarkesh awesome article on scaling laws. Many many incredible pointers/references/ideas well-articulated and communicated in this piece written as a dialogue.\nThe costs of size Okay, models are huge. How is it a problem ?\n‚Äî\u003e Training and inference costs increase with model size.\nWith crazy scale comes crazy numbers\nCompute, energy, memory, etc There are several costs to consider. Some a causally linked like computation and energy.\nCosts to consider when training / running AI models :\nMemory to load models\nFLOPs to train or run inferences ~ energy, latency, throughput\nCluster maintenance becomes a challenge ~ infrastructure, skills, time\nThe larger the model the more ressources are needed to both train and simply run an inference. In terms of computation, costs for a single pass of training and or a single batch of inference are roughly 6 proportional to the model size i.e its parameter count.\nMental model on ‚Äúcompute‚Äù :\n$$\\text{compute} \\sim \\text{data} \\times \\text{params} \\sim \\text{params}$$\nSee below the consequence of larger models (and larger training datasets) on compute costs :\nCompute is driven by model and dataset sizes (source : Epoch AI \u003c3)\nThe computation cost directly translates to the more fundamental energy cost. As a very concrete example laid out in the llama 2 paper, the training of the Llama 2 series required 3,311,616 \u003e 3 billions GPU hours - and it‚Äôs your A100-with-80-GB kind of GPU hours. They estimated that this amount of compute corresponds to 539 tons of CO2e emitted. The largest llama model - at 70 billions parameters - required ~1,720,000 GPU hours ‚Äî\u003e think a cluster of 6,000 A100s working full-time for 12 days.\nKarpathy in his intro talk to LLMs, these are ‚Äúrookie numbers‚Äù by today‚Äôs best models standard, ‚Äúoff by a factor of 10 or more‚Äù ‚Ä¶\nKarpathy : these are ‚Äò‚Äúrookie numbers‚Äù, ‚Äúoff by a factor of 10 or more‚Äù\nTraining models at this incredible scale requires crazy infrastructures. For the llama series, meta is leveraging 2 clusters totaling 48 000 GPUs 7. Maintaining the good health of such clusters becomes a technical challenge as discussed here for example.\nEstimating memory requirement As a first estimation :\n$$ \\text{Total mem. required} = \\text{#params} \\times \\text{(mem. per param)} $$\nStraightforward : to load the model, you have to load every parameters. Thus memory required is the sum of the memory required to load a single parameter.\nHow can we know the memory requirement of a single weight ? This is directly related to the ‚Äúprecision‚Äù chosen to represent this number in the computer. In scientific computing where a lot of precision is needed, real number are often represented with 64 bits. In Machine learning, it‚Äôs often 32 bits as this is already sufficient.\nI will not dive into the specifics of representing numbers in the computer here. This will be the topic of the next section. But knowing how many bits are used to store a single parameter, one can easy convert this to a number of bytes - 1 bytes = 8 bits. Finally, 1 billion bytes is exactly 1 GB. Given that models are often in the billions of parameters in size, you can easily estimate the memory requirement to hold the entire model in memory (be it the disk, the RAM or a GPU memory).\nModel compression via quantization General idea Model are getting larger and larger. On the one hand, larger sizes are driving AI progress. On the other hand, larger sizes come with costs :\nLoading models requires a lot of memory\nRunning/training models requires doing a ton of compute - FLOPs\nMemory and compute being fundamentally related to energy - and as a consequence money.\nIn order to benefit from powerful AI while diminishing the costs, one could try to compress models. The idea is would be to attack the problem at its root : model sizes.\nLLMs as an example As Andrej Karpathy so clearly summarizes, LLMs are lossy compressions of their training dataset. This is true in general in AI where one can think of models as big curves fitted on a dataset - think polynomial interpolation - Fran√ßois Chollet explains this all the time, here for instance. Thus data is compressed into the model through its parameters and computation graph.\nGiven all the general knowledge that models encapsulate - think your experience with chatGPT - you may believe that this compression is extremely precise and that tweaking a couple of parameters may throw everything off ‚Ä¶\nHowever this is absolutely not the case.\nAmong the billions of parameters, many if not most are rarely being activated and their contribution to performance - next word prediction for instance - is very negligible.\nThe extent to which some parameters are negligible can be quantified and one may try to simply remove these negligible weights. By removing useless parameters, models get lighter. This process is called pruning. Pruning - individual weights or even entire layers - is one class of compression algorithm. Quantization is another one.\nQuantization algorithms typically do not remove weights nor layers directly. The structure of the model stays the same. The number of parameters stays the same.\nWhat is going to change is how much memory is used to store each weight.\nFirst example : na√Øve 32-bit ‚Äî\u003e 16-bit The specifics of data types and how numbers are stored will be covered in the next section. For now, accept that numbers in a computer can be stored and used in computation using either :\n32 bits = 4 bytes of memory per number\n16 bits = 2 bytes of memory per number\nUsually, AI models will have their parameters represented in the computer using the 1st method : 32-bit precision. This allow for very precise calculations as we will explore in the next section.\nHowever, this precision may not be needed to run models. Let‚Äôs try to convert a model from 32-bit precision to 16-bit precision.\n‚Äî\u003e As we cut off memory requirement by half for each parameter, the memory requirement for the model as a whole will also be slashed in half.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer import torch torch.manual_seed(2) # Simple quantization algo on a \"small\" gpt-2 --\u003e gpu not needed device = \"cpu\" # load model and its tokenizer (in ram) model_id = \"gpt2\" model = AutoModelForCausalLM.from_pretrained(model_id).to(device) tokenizer = AutoTokenizer.from_pretrained(model_id) print(\"--------------------------------------\") print(\"What's the model size in (mega)bytes ?\") model_size_in_bytes = model.get_memory_footprint() print(f\"Model memory footprint : {model_size_in_bytes/1e6:.2f} MB\") print(\"---------------------------------------------------------\") print(\"Quantizing 32-bit --\u003e 16-bit the first layer of the model\") # extract the first layer weights (cf Maxime Labonne's article code example) weights = model.transformer.h[0].attn.c_attn.weight.data # Layer size in bytes layer_size_in_bytes = weights.element_size() * weights.nelement() print(f\"Original layer memory footprint : {layer_size_in_bytes/1e6:.2f} MB\") print(f\"Original data type : {weights.dtype}\") print(\"Original weights: \\n\", weights) # 32-bit --\u003e 16-bit weights_q = weights.to(torch.float16) # Lighter layer : size cut in half ? new_layer_size_bytes = weights_q.element_size() * weights_q.nelement() print(f\"Quantized layer memory footprint : {new_layer_size_bytes/1e6:.2f} MB\") print(f\"New data type : {weights_q.dtype}\") print(\"New quantized weights: \\n\", weights) print(\"---------------------------------------------\") print(\"Quantizing 32-bit --\u003e 16-bit the entire model \") # Layer by layer quantization for layer in model.parameters(): layer.data = layer.data.to(torch.float16) # New model memory footprint print(\"What's the quantized model size in (mega)bytes ?\") new_memory_footprint_bytes = model.get_memory_footprint() print(f\"Quantized model memory footprint : {new_memory_footprint_bytes/1e6:.2f} MB\") -------------------------------------- What's the model size in (mega)bytes ? Model memory footprint : 510.34 MB --------------------------------------------------------- Quantizing 32-bit --\u003e 16-bit the first layer of the model Original layer memory footprint : 7.08 MB Original data type : torch.float32 Original weights: tensor([[-0.4738, -0.2614, -0.0978, ..., 0.0513, -0.0584, 0.0250], [ 0.0874, 0.1473, 0.2387, ..., -0.0525, -0.0113, -0.0156], [ 0.0039, 0.0695, 0.3668, ..., 0.1143, 0.0363, -0.0318], ..., [-0.2592, -0.0164, 0.1991, ..., 0.0095, -0.0516, 0.0319], [ 0.1517, 0.2170, 0.1043, ..., 0.0293, -0.0429, -0.0475], [-0.4100, -0.1924, -0.2400, ..., -0.0046, 0.0070, 0.0198]]) Quantized layer memory footprint : 3.54 MB New data type : torch.float16 New quantized weights: tensor([[-0.4738, -0.2614, -0.0978, ..., 0.0513, -0.0584, 0.0250], [ 0.0874, 0.1473, 0.2387, ..., -0.0525, -0.0113, -0.0156], [ 0.0039, 0.0695, 0.3668, ..., 0.1143, 0.0363, -0.0318], ..., [-0.2592, -0.0164, 0.1991, ..., 0.0095, -0.0516, 0.0319], [ 0.1517, 0.2170, 0.1043, ..., 0.0293, -0.0429, -0.0475], [-0.4100, -0.1924, -0.2400, ..., -0.0046, 0.0070, 0.0198]]) --------------------------------------------- Quantizing 32-bit --\u003e 16-bit the entire model What's the quantized model size in (mega)bytes ? Quantized model memory footprint : 261.46 MB This piece of code shows you how you can very easily download a model from the Hugging Face hub to your local machine. Then, you can leverage built-in utils from the transformers and torch librairies to look at and change the model size in bytes.\nInitially, weights are stored in the computer using 32-bit precision. This code forces the weights from 32 bits to a 16 bits representation in memory. You can see that GPT-2 orignally weights in at ~500 MB. After quantization, it‚Äôs at ~250 MB - half the size as anticipated.\nThis transition from 32-bit to 16-bit precision is analogous to truncating a number in physics to account for the significance of digits. Essentially, we are saying :\n$$\\pi \\simeq 3.141592654 \\simeq 3.1415$$\nfor every parameters of the model. More or less truncating the ‚Äúprecision‚Äù by half 8. See how you don‚Äôt even notice the change in precision with the prints ! But still, precision has been diminished.\nHow is the performance affected ? Let‚Äôs try to benchmark this with :\nQualitative evaluation with a simple inference on a given prompt\nQuantitative measurement of ‚Äúnext token prediction‚Äù with the model perplexity on a given dataset\nN-bit precision and memory footprint calculation will be explained later in the post. Basically, each parameter requires 32 bits / 8 bits per byte = 4 bytes of memory. Thus 70 billions of such parameters amount to 70B * 4 bytes = 280 GB of memory.¬†‚Ü©Ô∏é\nConsidering a ‚Äúsimple‚Äù run where on tries to load the entire model in a single GPU at once. However there are many tactics to run larger model on a GPU than one would expect given the amount of VRAM available (for instance off-loading some layers onto the RAM or even the disk to alleviate the memory pressure on the GPU)¬†‚Ü©Ô∏é\n‚Äú117M‚Äù ? Given gpt2 x10 gpt1, must be ~100M¬†‚Ü©Ô∏é\nBig caveat here as we are not comparing apples to apples. GPT-1, 2 and 3 are ‚Äòdense‚Äô as opposed to the ‚Äòsparse‚Äô MoE architecture that GPT-4 is believed to be. Same kind of architecture than the popular Mixtrals for instance, where only a subset of all the weights are active in a given inference round.¬†‚Ü©Ô∏é\nI have not read it thoroughly. I just discovered it while writing this post. But as always with EpochAI work it looks great.¬†‚Ü©Ô∏é\nThe specifics depends on your training/inference strategy.¬†‚Ü©Ô∏é\n2 clusters of 24 000 GPU. See llama 3 introduction post (section ‚ÄúScaling up pretraining‚Äù).¬†‚Ü©Ô∏é\nHow the precision is affected depends on how you use your 32 or 16 bits. More on that in the data type section.¬†‚Ü©Ô∏é\n",
  "wordCount" : "2583",
  "inLanguage": "en",
  "datePublished": "2024-05-23T20:31:19+02:00",
  "dateModified": "2024-05-23T20:31:19+02:00",
  "author":{
    "@type": "Person",
    "name": "Nicolas Pellerin"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://nicolaspllr1.github.io/posts/quantization_intro/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Nicolas' Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://nicolaspllr1.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://nicolaspllr1.github.io/" accesskey="h" title="Nicolas&#39; Blog (Alt + H)">Nicolas&#39; Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://nicolaspllr1.github.io/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://nicolaspllr1.github.io/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://nicolaspllr1.github.io/about/" title="About">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      What is quantization in modern AI ?
    </h1>
    <div class="post-meta"><span title='2024-05-23 20:31:19 +0200 CEST'>May 23, 2024</span>&nbsp;¬∑&nbsp;13 min&nbsp;¬∑&nbsp;Nicolas Pellerin

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#what-is-quantization-" aria-label="What is quantization ?">What is quantization ?</a><ul>
                        
                <li>
                    <a href="#first-definition" aria-label="First definition">First definition</a></li>
                <li>
                    <a href="#case-study--70b-model" aria-label="Case study : 70B model">Case study : 70B model</a></li></ul>
                </li>
                <li>
                    <a href="#modern-context" aria-label="Modern context">Modern context</a><ul>
                        
                <li>
                    <a href="#plus-size-ai" aria-label="Plus-size AI">Plus-size AI</a></li>
                <li>
                    <a href="#scaling-laws" aria-label="Scaling laws">Scaling laws</a></li></ul>
                </li>
                <li>
                    <a href="#the-costs-of-size" aria-label="The costs of size">The costs of size</a><ul>
                        
                <li>
                    <a href="#compute-energy-memory-etc" aria-label="Compute, energy, memory, etc">Compute, energy, memory, etc</a></li>
                <li>
                    <a href="#estimating-memory-requirement" aria-label="Estimating memory requirement">Estimating memory requirement</a></li></ul>
                </li>
                <li>
                    <a href="#model-compression-via-quantization" aria-label="Model compression via quantization">Model compression via quantization</a><ul>
                        
                <li>
                    <a href="#general-idea" aria-label="General idea">General idea</a></li>
                <li>
                    <a href="#llms-as-an-example" aria-label="LLMs as an example">LLMs as an example</a></li>
                <li>
                    <a href="#first-example--na%c3%afve-32-bit--16-bit" aria-label="First example : na√Øve 32-bit ‚Äî&gt; 16-bit">First example : na√Øve 32-bit ‚Äî&gt; 16-bit</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p><img loading="lazy" src="/images/cute_llama.webp" alt="Cute llama lifting weights - courtesy of DALL-E 3"  />
</p>
<h2 id="what-is-quantization-">What is quantization ?<a hidden class="anchor" aria-hidden="true" href="#what-is-quantization-">#</a></h2>
<h3 id="first-definition">First definition<a hidden class="anchor" aria-hidden="true" href="#first-definition">#</a></h3>
<hr>
<p><em>Quantization</em> refers to a broad class of algorithms. Their goal is to reduce the memory footprint of models while retaining as much performance as possible.</p>
<p>$$
\text{Quantization} \ \approx \ \searrow \ \text{memory footprint of models}
$$</p>
<p>More precisely, this reduction in memory size will be achieved by compressing models <em>weights</em> - or <em>parameters</em> if you prefer.</p>
<hr>
<h3 id="case-study--70b-model">Case study : 70B model<a hidden class="anchor" aria-hidden="true" href="#case-study--70b-model">#</a></h3>
<p>Consider the new Llama 3 70B.</p>
<p>If you load it in 32-bit precision on your GPU <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>, it will require around (32 / 8) * 70 = 280 GB of VRAM ü•∂. For regular folks with access to 1 or 2 consumer GPUs at best, this memory cost is prohibitive and the model cannot be run <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>.</p>
<p>This is where quantization kicks in to lower the memory required to run models.</p>
<p>Before we dive into data types, memory footprint calculations and specific quantization algorithms such as GPTQ, AWQ or HQQ, let&rsquo;s understand why quantization in particular and model compression in general have become critical in modern AI.</p>
<h2 id="modern-context">Modern context<a hidden class="anchor" aria-hidden="true" href="#modern-context">#</a></h2>
<h3 id="plus-size-ai">Plus-size AI<a hidden class="anchor" aria-hidden="true" href="#plus-size-ai">#</a></h3>
<p>AI models have been getting larger and larger.</p>
<p>Epoch AI has a nice <a href="https://epochai.org/blog/machine-learning-model-sizes-and-the-parameter-gap">paper</a> exploring this trend + their <a href="https://epochai.org/blog/compute-trends">dataset</a> is regularly updated. Look at the evolution of the parameter count from 1950 to 2024 :</p>
<p><img loading="lazy" src="/images/epochAI_model_sizes.png" alt="Evolution of model sizes over time by EpochAI"  />
</p>
<p>Pay attention to the y-axis log scale. It indicates the parameter count order of magnitude - OOM. Notice how the slope is increasing.  On this figure, every vertical increment represent a gain of a full order of magnitude !</p>
<p>Meaning across the board,  models have been gaining orders of magnitudes more parameters and this increase in size has been accelerating</p>
<p><strong>Natural language processing</strong> Consider the GPT series trained by OpenAI :</p>

<figure><img src="/images/GPTs_timelmine_summary.png"
    alt="GPTs timeline"><figcaption>
      <p>Quick drawing I made - these GPTs sizes are well know as they were disclosed in OpenAI‚Äôs technical reports/papers/blog posts</p>
    </figcaption>
</figure>

<p>Gaining 3 OOMs was achieved in ~ 2 years.</p>

<figure><img src="/images/gpt_series_OOMs_screen.png"
    alt="GPTs sizes"><figcaption>
      <p>Quick drawing I made</p>
    </figcaption>
</figure>

<p>Mid 2018, OpenAI released GPT-1 with ~100M params <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>. Next in 2019, they progressively released the GPT-2 models. The largest GPT-2 at 1.5B params was 10x larger than GPT-1 . Next mid 2020 they announced GPT-3 - but did not release the weights this time üò¢ - with an outstanding size of 175B parameters. Meaning ~100x GPT-2 size and 10x larger than any dense model at the time. Finally in March 2024 they announced GPT-4. Although this time they did not publicly disclose its architecture, it is rumored to be a 1.7T MoE so again a 10x increase in size compared to GPT-3 <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>.</p>
<p>And it&rsquo;s not just OpenAI‚Äôs GPTs. This trend can be observed across the board in the modern NLP history :</p>
<table>
<thead>
<tr>
<th>Year</th>
<th>Model</th>
<th>Parameter Count</th>
</tr>
</thead>
<tbody>
<tr>
<td>1997</td>
<td>Original LSTM</td>
<td>~10k</td>
</tr>
<tr>
<td>2014</td>
<td><a href="https://arxiv.org/abs/1409.3215">Seq2Seq LSTM</a></td>
<td>~400M</td>
</tr>
<tr>
<td>2017</td>
<td><a href="https://arxiv.org/abs/1706.03762">Original Transformer</a></td>
<td>~200M</td>
</tr>
<tr>
<td>2018</td>
<td><a href="https://arxiv.org/abs/1810.04805">BERT Base - Large</a></td>
<td>110M -  340M</td>
</tr>
<tr>
<td>Early 2023</td>
<td>Llama 1 series</td>
<td>7B, 13B, 33B, 65B</td>
</tr>
<tr>
<td>Summer 2023</td>
<td>Llama 2 series</td>
<td>7B, 13B, 70B</td>
</tr>
<tr>
<td>April 2024</td>
<td>Llama 3 series</td>
<td>8B, 70B, 400B (not released yet)</td>
</tr>
</tbody>
</table>
<p>In 2024, almost all models have a parameter count in the billions ! Models having less than 10 billions parameters are often considered &ldquo;small&rdquo;  ! For instance, the release of llama 3 8B has been much appreciated as its size makes it relatively small in the modern context of LLMs and it has been trained a lot. Making smaller models more capable is a recent trend that seems to be gaining traction. Andrej Karpathy for one has been pushing for smaller yet capable models and the recent <a href="https://arxiv.org/abs/2404.14219">Phi-3</a> model for example drives in this direction.</p>
<p><strong>Computer vision</strong> Same thing in computer vision although the latest models are not as large as the largest LLMs. Let‚Äôs compress aggressively modern computer vision history into a couple of milestone models : LeNet ‚Äî&gt;AlexNet ‚Äî&gt; ResNets, YOLOs ‚Äî&gt; ViTs</p>

<figure><img src="/images/landmark_models_cv_screen.png"
    alt="Landmark models in computer vision - timeline"><figcaption>
      <p>Timeline I made. See the increasing number of parameters :)</p>
    </figcaption>
</figure>

<p>In short, models sizes across the board in the AI landscape have been exploding.     And why you may ask ?</p>
<h3 id="scaling-laws">Scaling laws<a hidden class="anchor" aria-hidden="true" href="#scaling-laws">#</a></h3>
<p>One major force that is driving this explosion is scaling laws.</p>
<p>I won&rsquo;t deep dive into this fascinating topic here - maybe in a future post :) .</p>
<p>If you want to get started, EpochAI literature <a href="https://epochai.org/blog/scaling-laws-literature-review">review</a> looks great <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>.</p>
<p>On a personal note, I would recommend paper-wise reading OpenAI&rsquo;s 2020 publications where they study <a href="https://arxiv.org/abs/2001.08361">scaling laws for GPTs performing language</a> modeling up to 1B models, and then explore how their findings <a href="https://arxiv.org/abs/2010.14701">generalize to other AI tasks</a>.</p>

<figure><img src="/images/openAI_power_laws.png"
    alt="OpenAI scaling laws - power law"><figcaption>
      <p>Scaling laws beautifully visualized - in OpenAI‚Äôs first paper mentioned above</p>
    </figcaption>
</figure>

<p>Finally, the 2022 <a href="https://arxiv.org/abs/2203.15556">Chinchilla paper</a> which provides more precise scaling laws than OpenAI&rsquo;s and estimates ‚Äúcompute optimal‚Äù training recipes.</p>
<p>And obviously you should read <a href="https://www.dwarkeshpatel.com/p/will-scaling-work">Dwarkesh awesome article on scaling laws</a>. Many many incredible pointers/references/ideas well-articulated and communicated in this piece written as a dialogue.</p>
<h2 id="the-costs-of-size">The costs of size<a hidden class="anchor" aria-hidden="true" href="#the-costs-of-size">#</a></h2>
<p>Okay, models are huge. How is it a problem ?</p>
<p>‚Äî&gt; Training and inference costs increase with model size.</p>
<p>With crazy scale comes crazy numbers</p>
<h3 id="compute-energy-memory-etc">Compute, energy, memory, etc<a hidden class="anchor" aria-hidden="true" href="#compute-energy-memory-etc">#</a></h3>
<p>There are several costs to consider. Some a causally linked like computation and energy.</p>
<p>Costs to consider when training / running AI models :</p>
<ul>
<li>
<p>Memory to load models</p>
</li>
<li>
<p>FLOPs to train or run inferences ~ energy, latency, throughput</p>
</li>
<li>
<p>Cluster maintenance becomes a challenge ~ infrastructure, skills, time</p>
</li>
</ul>
<p>The larger the model the more ressources are needed to both train and simply run an inference. In terms of computation, costs for a single pass of training and or a single batch of inference are roughly <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup> proportional to the model size i.e its parameter count.</p>
<p>Mental model on ‚Äúcompute‚Äù :</p>
<p>$$\text{compute} \sim \text{data} \times \text{params} \sim \text{params}$$</p>
<p>See below the consequence of larger models (and larger training datasets) on compute costs :</p>

<figure><img src="/images/compute-trends.png"
    alt="EpochAI compute trend figure"><figcaption>
      <p>Compute is driven by model and dataset sizes (source : Epoch AI &lt;3)</p>
    </figcaption>
</figure>

<p>The computation cost directly translates to the more fundamental energy cost. As a very concrete example laid out in the <a href="https://arxiv.org/abs/2307.09288">llama 2 paper</a>, the training of the Llama 2 series required  3,311,616 &gt; 3 billions GPU hours - and it‚Äôs your A100-with-80-GB kind of GPU hours. They estimated that this amount of compute corresponds to 539 tons of CO2e emitted. The largest llama model - at 70 billions parameters - required ~1,720,000 GPU hours ‚Äî&gt; think a cluster of 6,000 A100s working full-time for 12 days.</p>
<p>Karpathy in his <a href="https://youtu.be/zjkBMFhNj_g?si=dqVGfCspOohoCps8&amp;t=364">intro talk to LLMs</a>, these are ‚Äúrookie numbers‚Äù by today‚Äôs best models standard, ‚Äúoff by a factor of 10 or more‚Äù ‚Ä¶</p>

<figure><img src="/images/karpathy_llama2_rookie_numbers.png"
    alt="Screenshot from Karpathy&#39;s video - llama2 rookie numbers"><figcaption>
      <p>Karpathy : these are ‚Äò‚Äúrookie numbers‚Äù, ‚Äúoff by a factor of 10 or more‚Äù</p>
    </figcaption>
</figure>

<p>Training models at this incredible scale requires crazy infrastructures. For the llama series, meta is leveraging 2 clusters totaling 48 000 GPUs <sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>. Maintaining the good health of such clusters becomes a technical challenge as discussed <a href="https://twitter.com/karpathy/status/1765424847705047247">here for example</a>.</p>
<h3 id="estimating-memory-requirement">Estimating memory requirement<a hidden class="anchor" aria-hidden="true" href="#estimating-memory-requirement">#</a></h3>
<p>As a first estimation :</p>
<p>$$
\text{Total mem. required} = \text{#params} \times \text{(mem. per param)}
$$</p>
<p>Straightforward : to load the model, you have to load every parameters. Thus memory required is the sum of the memory required to load a single parameter.</p>
<p>How can we know the memory requirement of a single weight ? This is directly related to the ‚Äúprecision‚Äù chosen to represent this number in the computer. In scientific computing where a lot of precision is needed, real number are often represented with 64 bits. In Machine learning, it‚Äôs often 32 bits as this is already sufficient.</p>
<p>I will not dive into the specifics of representing numbers in the computer here. This will be the topic of the next section. But knowing how many bits are used to store a single parameter, one can easy convert this to a number of bytes - 1 bytes = 8 bits. Finally, 1 billion bytes is exactly 1 GB. Given that models are often in the billions of parameters in size, you can easily estimate the memory requirement to hold the entire model in memory (be it the disk, the RAM or a GPU memory).</p>
<h2 id="model-compression-via-quantization">Model compression via quantization<a hidden class="anchor" aria-hidden="true" href="#model-compression-via-quantization">#</a></h2>
<h3 id="general-idea">General idea<a hidden class="anchor" aria-hidden="true" href="#general-idea">#</a></h3>
<p>Model are getting larger and larger. On the one hand, larger sizes are driving AI progress. On the other hand,  larger sizes come with costs :</p>
<ul>
<li>
<p>Loading models requires a lot of memory</p>
</li>
<li>
<p>Running/training models requires doing a ton of compute - FLOPs</p>
</li>
<li>
<p>Memory and compute being fundamentally related to energy - and as a consequence money.</p>
</li>
</ul>
<p>In order to benefit from powerful AI while diminishing the costs, one could try to compress models. The idea is would be to attack the problem at its root : model sizes.</p>
<h3 id="llms-as-an-example">LLMs as an example<a hidden class="anchor" aria-hidden="true" href="#llms-as-an-example">#</a></h3>
<p>As Andrej Karpathy so clearly <a href="https://www.youtube.com/watch?v=zjkBMFhNj_g&amp;t=1683s&amp;ab_channel=AndrejKarpathy">summarizes</a>, LLMs are lossy compressions of their training dataset. This is true in general in AI where one can think of models as big curves fitted on a dataset - think <a href="https://en.wikipedia.org/wiki/Polynomial_interpolation">polynomial interpolation</a> - Fran√ßois Chollet explains this all the time, <a href="https://twitter.com/fchollet/status/1756018992282746981">here for instance</a>. Thus data is compressed into the model through its parameters and computation graph.</p>
<p>Given all the general knowledge that models encapsulate - think your experience with chatGPT - you may believe that this compression is extremely precise and that tweaking a couple of parameters may throw everything off ‚Ä¶</p>
<p>However this is absolutely not the case.</p>
<p>Among the billions of parameters, many if not most are rarely being activated and their contribution to performance - next word prediction for instance - is very negligible.</p>
<p>The extent to which some parameters are negligible can be quantified and one may try to simply remove these negligible weights. By removing useless parameters, models get lighter. This process is called pruning. Pruning - individual weights or even <a href="https://arxiv.org/abs/2403.17887">entire layers</a> -  is one class of compression algorithm. Quantization is another one.</p>
<p>Quantization algorithms typically do not remove weights nor layers directly. The structure of the model stays the same. The number of parameters stays the same.</p>
<p>What is going to change is how much memory is used to store each weight.</p>
<h3 id="first-example--na√Øve-32-bit--16-bit">First example : na√Øve 32-bit ‚Äî&gt; 16-bit<a hidden class="anchor" aria-hidden="true" href="#first-example--na√Øve-32-bit--16-bit">#</a></h3>
<p>The specifics of data types and how numbers are stored will be covered in the next section. For now, accept that numbers in a computer can be stored and used in computation using either :</p>
<ul>
<li>
<p>32 bits = 4 bytes of memory per number</p>
</li>
<li>
<p>16 bits = 2 bytes of memory per number</p>
</li>
</ul>
<p>Usually, AI models will have their parameters represented in the computer using the 1st method : 32-bit precision. This allow for very precise calculations as we will explore in the next section.</p>
<p>However, this precision may not be needed to run models. Let‚Äôs try to convert a model from 32-bit precision to 16-bit precision.</p>
<p>‚Äî&gt; As we cut off memory requirement by half for each parameter, the memory requirement for the model as a whole will also be slashed in half.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> AutoModelForCausalLM, AutoTokenizer
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>torch<span style="color:#f92672">.</span>manual_seed(<span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Simple quantization algo on a &#34;small&#34; gpt-2 --&gt; gpu not needed</span>
</span></span><span style="display:flex;"><span>device <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;cpu&#34;</span> 
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># load model and its tokenizer (in ram)</span>
</span></span><span style="display:flex;"><span>model_id <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;gpt2&#34;</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> AutoModelForCausalLM<span style="color:#f92672">.</span>from_pretrained(model_id)<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>tokenizer <span style="color:#f92672">=</span> AutoTokenizer<span style="color:#f92672">.</span>from_pretrained(model_id)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;--------------------------------------&#34;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;What&#39;s the model size in (mega)bytes ?&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model_size_in_bytes <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>get_memory_footprint()
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Model memory footprint : </span><span style="color:#e6db74">{</span>model_size_in_bytes<span style="color:#f92672">/</span><span style="color:#ae81ff">1e6</span><span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> MB&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;---------------------------------------------------------&#34;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Quantizing 32-bit --&gt; 16-bit the first layer of the model&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># extract the first layer weights (cf Maxime Labonne&#39;s article code example)</span>
</span></span><span style="display:flex;"><span>weights <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>transformer<span style="color:#f92672">.</span>h[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>attn<span style="color:#f92672">.</span>c_attn<span style="color:#f92672">.</span>weight<span style="color:#f92672">.</span>data
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Layer size in bytes</span>
</span></span><span style="display:flex;"><span>layer_size_in_bytes <span style="color:#f92672">=</span> weights<span style="color:#f92672">.</span>element_size() <span style="color:#f92672">*</span> weights<span style="color:#f92672">.</span>nelement()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Original layer memory footprint : </span><span style="color:#e6db74">{</span>layer_size_in_bytes<span style="color:#f92672">/</span><span style="color:#ae81ff">1e6</span><span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> MB&#34;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Original data type : </span><span style="color:#e6db74">{</span>weights<span style="color:#f92672">.</span>dtype<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Original weights: </span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>, weights)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 32-bit --&gt; 16-bit</span>
</span></span><span style="display:flex;"><span>weights_q <span style="color:#f92672">=</span> weights<span style="color:#f92672">.</span>to(torch<span style="color:#f92672">.</span>float16)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Lighter layer : size cut in half ?</span>
</span></span><span style="display:flex;"><span>new_layer_size_bytes <span style="color:#f92672">=</span> weights_q<span style="color:#f92672">.</span>element_size() <span style="color:#f92672">*</span> weights_q<span style="color:#f92672">.</span>nelement()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Quantized layer memory footprint : </span><span style="color:#e6db74">{</span>new_layer_size_bytes<span style="color:#f92672">/</span><span style="color:#ae81ff">1e6</span><span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> MB&#34;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;New data type : </span><span style="color:#e6db74">{</span>weights_q<span style="color:#f92672">.</span>dtype<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;New quantized weights: </span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>, weights)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;---------------------------------------------&#34;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Quantizing 32-bit --&gt; 16-bit the entire model &#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Layer by layer quantization</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> layer <span style="color:#f92672">in</span> model<span style="color:#f92672">.</span>parameters():
</span></span><span style="display:flex;"><span>    layer<span style="color:#f92672">.</span>data <span style="color:#f92672">=</span> layer<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>to(torch<span style="color:#f92672">.</span>float16)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># New model memory footprint</span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;What&#39;s the quantized model size in (mega)bytes ?&#34;</span>)
</span></span><span style="display:flex;"><span>new_memory_footprint_bytes <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>get_memory_footprint()
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Quantized model memory footprint : </span><span style="color:#e6db74">{</span>new_memory_footprint_bytes<span style="color:#f92672">/</span><span style="color:#ae81ff">1e6</span><span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> MB&#34;</span>)
</span></span></code></pre></div><pre tabindex="0"><code class="language-terminal" data-lang="terminal">--------------------------------------

What&#39;s the model size in (mega)bytes ?
Model memory footprint : 510.34 MB

---------------------------------------------------------
Quantizing 32-bit --&gt; 16-bit the first layer of the model

Original layer memory footprint : 7.08 MB
Original data type : torch.float32
Original weights: 
 tensor([[-0.4738, -0.2614, -0.0978,  ...,  0.0513, -0.0584,  0.0250],
        [ 0.0874,  0.1473,  0.2387,  ..., -0.0525, -0.0113, -0.0156],
        [ 0.0039,  0.0695,  0.3668,  ...,  0.1143,  0.0363, -0.0318],
        ...,
        [-0.2592, -0.0164,  0.1991,  ...,  0.0095, -0.0516,  0.0319],
        [ 0.1517,  0.2170,  0.1043,  ...,  0.0293, -0.0429, -0.0475],
        [-0.4100, -0.1924, -0.2400,  ..., -0.0046,  0.0070,  0.0198]])

Quantized layer memory footprint : 3.54 MB
New data type : torch.float16
New quantized weights: 
 tensor([[-0.4738, -0.2614, -0.0978,  ...,  0.0513, -0.0584,  0.0250],
        [ 0.0874,  0.1473,  0.2387,  ..., -0.0525, -0.0113, -0.0156],
        [ 0.0039,  0.0695,  0.3668,  ...,  0.1143,  0.0363, -0.0318],
        ...,
        [-0.2592, -0.0164,  0.1991,  ...,  0.0095, -0.0516,  0.0319],
        [ 0.1517,  0.2170,  0.1043,  ...,  0.0293, -0.0429, -0.0475],
        [-0.4100, -0.1924, -0.2400,  ..., -0.0046,  0.0070,  0.0198]])

---------------------------------------------
Quantizing 32-bit --&gt; 16-bit the entire model 

What&#39;s the quantized model size in (mega)bytes ?
Quantized model memory footprint : 261.46 MB
</code></pre><p>This piece of code shows you how you can very easily download a model from the Hugging Face hub to your local machine. Then, you can leverage built-in utils from the transformers and torch librairies to look at and change the model size in bytes.</p>
<p>Initially, weights are stored in the computer using 32-bit precision. This code forces the weights from 32 bits to a 16 bits representation in memory. You can see that GPT-2 orignally weights in at ~500 MB. After quantization, it&rsquo;s at ~250 MB - half the size as anticipated.</p>
<p>This transition from 32-bit to 16-bit precision is analogous to truncating a number in physics to account for the significance of digits. Essentially, we are saying :</p>
<p>$$\pi \simeq 3.141592654 \simeq 3.1415$$</p>
<p>for every parameters of the model. More or less truncating the ‚Äúprecision‚Äù by half <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>. See how you don‚Äôt even notice the change in precision with the prints ! But still, precision has been diminished.</p>
<p>How is the performance affected ? Let‚Äôs try to benchmark this with :</p>
<ul>
<li>
<p>Qualitative evaluation with a simple inference on a given prompt</p>
</li>
<li>
<p>Quantitative measurement of ‚Äúnext token prediction‚Äù with the model perplexity on a given dataset</p>
</li>
</ul>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>N-bit precision and memory footprint calculation will be explained later in the post. Basically, each parameter requires 32 bits / 8 bits per byte = 4 bytes of memory. Thus 70 billions of such parameters amount to 70B * 4 bytes = 280 GB of memory.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Considering a ‚Äúsimple‚Äù run where on tries to load the entire model in a single GPU at once. However there are many tactics to run larger model on a GPU than one would expect given the amount of VRAM available (for instance off-loading some layers onto the RAM or even the disk to alleviate the memory pressure on the GPU)&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>&ldquo;117M&rdquo; ? Given gpt2 x10 gpt1, must be ~100M&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Big caveat here as we are not comparing apples to apples. GPT-1, 2 and 3 are  &lsquo;dense&rsquo;  as opposed to the &lsquo;sparse&rsquo; MoE architecture   that GPT-4 is believed to be. Same  kind of architecture than the popular Mixtrals for instance, where only a subset of all the weights are active in a given inference round.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>I have not read it thoroughly. I just discovered it while writing this post. But as always with EpochAI work it looks great.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>The specifics depends on your training/inference strategy.&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>2 clusters of 24 000 GPU. See llama 3 introduction post  (section ‚ÄúScaling up pretraining‚Äù).&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>How the precision is affected depends on how you use your 32 or 16 bits. More on that in the data type section.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share What is quantization in modern AI ? on x"
            href="https://x.com/intent/tweet/?text=What%20is%20quantization%20in%20modern%20AI%20%3f&amp;url=https%3a%2f%2fnicolaspllr1.github.io%2fposts%2fquantization_intro%2f&amp;hashtags=">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share What is quantization in modern AI ? on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fnicolaspllr1.github.io%2fposts%2fquantization_intro%2f&amp;title=What%20is%20quantization%20in%20modern%20AI%20%3f&amp;summary=What%20is%20quantization%20in%20modern%20AI%20%3f&amp;source=https%3a%2f%2fnicolaspllr1.github.io%2fposts%2fquantization_intro%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share What is quantization in modern AI ? on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fnicolaspllr1.github.io%2fposts%2fquantization_intro%2f&title=What%20is%20quantization%20in%20modern%20AI%20%3f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share What is quantization in modern AI ? on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fnicolaspllr1.github.io%2fposts%2fquantization_intro%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share What is quantization in modern AI ? on whatsapp"
            href="https://api.whatsapp.com/send?text=What%20is%20quantization%20in%20modern%20AI%20%3f%20-%20https%3a%2f%2fnicolaspllr1.github.io%2fposts%2fquantization_intro%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share What is quantization in modern AI ? on telegram"
            href="https://telegram.me/share/url?text=What%20is%20quantization%20in%20modern%20AI%20%3f&amp;url=https%3a%2f%2fnicolaspllr1.github.io%2fposts%2fquantization_intro%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share What is quantization in modern AI ? on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=What%20is%20quantization%20in%20modern%20AI%20%3f&u=https%3a%2f%2fnicolaspllr1.github.io%2fposts%2fquantization_intro%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="https://nicolaspllr1.github.io/">Nicolas&#39; Blog</a></span> ¬∑ 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
